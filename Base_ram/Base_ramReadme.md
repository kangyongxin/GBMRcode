
这个是ram版本，后来要把它还原回去

# 环境解析

Mspacman ram 环境的基本解读：testram.py 
先让它一直做一个动作，看看哪个是位置信息，发现前80回合无效

动作空间：九个动作 

编号|动作内容|备注
-|:-:|:-
0|向左|
1|向上|但是也是跑出格子能向上的时候才向上
2|向右|
3|向左|似乎向左走这个动作是默认的
4|向下|也同样，是先向左再找可能的机会向下
5|右上|先向右走，再向上走，也可以理解为左转直行，右转直行交替进行
6|左上|先左走，再上走，或者说，先左转再右转再左转交替
7|右下|先右走，再下走，这个就没法用左右转交替解释了
8|左下|先左走，再下走

肯定有一个是停顿，但是没找到是哪一个

这里面比较靠谱的是向右 2， 如果有那个维度是表示坐标的那么理想的情况应该是，它在20几个回合内是变化的，后来又不变了
（从90开始走到118就不动了）

动作1 会向上，猜想O[11]它门紧邻，但好像并不是。100左右向上，130 左右会停止。


状态空间：总共有128维，我们的显示是每行18个

编号|内容|备注
-|:-:|:-
O[10]|疑似智能体的横坐标|采用向右的动作，在90~112步之间有变化，且增量2左右
O[16]|疑似智能体的纵坐标|采用向上的动作，在100~130之间有变化，增量-3左右
O[30]|疑似某运动物体的横坐标|从头开始一直在以4左右的增量增长

用其他动作复盘一下，确认结果

生命值：3

# 基本流程

run_MsPacman_ram.py
去掉编解码部分，把节点的特征直接做成横纵坐标.



# 超参数调节
1. 相似度问题，超过多少的阈值算是相似？
key
 [[62. 98.]] 
 memory
 [array([[63., 98.]], dtype=float32), array([[ 64., 158.]], dtype=float32)] 
 similarity
 [<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.9999737]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.983983]], dtype=float32)>]
key
 [[59. 98.]] 
 memory
 [array([[63., 98.]], dtype=float32), array([[ 64., 158.]], dtype=float32)] 
 similarity
 [<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.9995673]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.9876921]], dtype=float32)>]

 key
 [[55. 98.]] 
 memory
 [array([[55., 98.]], dtype=float32), array([[ 64., 158.]], dtype=float32)] 
 similarity
 [<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.9920024]], dtype=float32)>]

所以这个阈值的设定不能太低，否则大家就都很像了，至少要0.999像才行。但是这在有编码网络的时候不能设置这么高。

2. 抽象图保留比例 

针对mspacman ram 中，由于节点并不是很多，可以尝试提高比例，目前是0.9

# 问题

1. 目前的Agent 在记忆重构过程中用的是最短路，而我们最开始期待的是通过reward 最大的路径进行优化，可是如何在记忆图中去评价两点之间reward最大的路径呢？我们似乎可以通过引入对比学习的方式来完成这件事。类似于一种后悔值的方法，假如在某个可能点我们采用了不同的路径到达目的地是否会有奖励增益，以此来评价该路径的是否可取。


2. 如果我们以表观特征（位置）做聚类，那么，聚类的结果必然是与表观相关，那么最终的结果就不会有很多奖励导向机制，所以我们可以尝试在各个节点之间传播的信号改变成奖励值（或者值函数）
ps: 这种信息传播并不改变最底层的奖励值，只是改变GNN的顶层设计。

3. 检索的时候用表观检索，信息传播和聚类的时候用奖励做辅助